{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5658aad4-933d-4e55-a00d-7841e5f41300",
   "metadata": {},
   "source": [
    "# CAMS-MOS\n",
    "Demonstration code to download and plot air-quality point forecasts from the CAMS Atmosphere Data Store. Adetailed description is available at the ECMWF documentation website: [CAMS Regional: European Air Quality Forecast Optimised at Observation Sites data documentation](https://confluence.ecmwf.int/display/CKB/CAMS+Regional%3A+European+Air+Quality+Forecast+Optimised+at+Observation+Sites+data+documentation). In this notebook we use the [CAMS European air quality forecasts optimised at observation sites](https://ads.atmosphere.copernicus.eu/cdsapp#!/dataset/cams-europe-air-quality-forecasts-optimised-at-observation-sites?tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255a2546-7bdf-4e3a-86b7-342fa8cb6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import cdsapi\n",
    "import zipfile\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from math import ceil, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac4771-12e0-42f4-b805-1dbc0e74f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def main(cdsapirc_file=None):\n",
    "  \n",
    "    # The data to download\n",
    "    request = {'variable': ['nitrogen_dioxide', 'ozone',\n",
    "                            'particulate_matter_2.5um',\n",
    "                            'particulate_matter_10um'],\n",
    "               'country': 'netherlands',\n",
    "               'type': ['raw', 'mos_optimised'],\n",
    "               'leadtime_hour': ['0-23', '24-47'],\n",
    "               'year': ['2024'],\n",
    "               'month': ['01'],\n",
    "               'day': ['27', '28', '29', '30', '31'],\n",
    "               'include_station_metadata': 'yes',\n",
    "               'format': 'zip'}\n",
    "  \n",
    "    # The stations to plot. If None, plot them all.\n",
    "    #stations = None\n",
    "    stations = ['NL00014']\n",
    "  \n",
    "    # Plotting preferences\n",
    "    style = {\n",
    "        'type': {\n",
    "            'raw': {'color': 'tab:blue'},\n",
    "            'mos': {'color': 'tab:orange'}\n",
    "        },\n",
    "        'leadtime_day': {\n",
    "            0: {'linestyle': 'solid'},\n",
    "            1: {'linestyle': 'dashed'}\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    # Download the data. Use a filename that depends on the request so we\n",
    "    # don't have to re-download if the data already exists.\n",
    "    data_file = data_filename(request)\n",
    "    if not os.path.exists(data_file):\n",
    "        get_data(request, data_file, cdsapirc_file)\n",
    "  \n",
    "    # Read the data\n",
    "    station_data, data = read_data(data_file, stations)\n",
    "  \n",
    "    # Make a plot for each station\n",
    "    for station in data.station_id.unique():\n",
    "  \n",
    "        # Extract station metadata for just this site. If there's more than one\n",
    "        # entry we take the latest one that's valid within this time period\n",
    "        sdata = station_data.loc[\n",
    "            (station_data.id == station) &\n",
    "            (station_data.date_start <= data.datetime.iloc[-1]) &\n",
    "            (station_data.date_end >= data.datetime.iloc[0])\n",
    "        ]\n",
    "        assert len(sdata), 'No metadata for site?'\n",
    "        sdata = sdata.iloc[-1, :]\n",
    "  \n",
    "        # Extract air quality data for just this site\n",
    "        adata = data[data.station_id == station]\n",
    "  \n",
    "        if len(adata) > 0:\n",
    "            station_plot(sdata, adata, style)\n",
    "        else:\n",
    "            print('No data for ' + station)\n",
    "  \n",
    "  \n",
    "def data_filename(request):\n",
    "    \"\"\"Return a data filename containing a hash which depends on the request\"\"\"\n",
    "    hash = hashlib.md5()\n",
    "    hash.update(json.dumps(request, sort_keys=True).encode())\n",
    "    return 'data_' + hash.hexdigest() + '.zip'\n",
    "  \n",
    "  \n",
    "def get_data(request, data_file, cdsapirc_ifile):\n",
    "    \"\"\"Download requested data from the ADS\"\"\"\n",
    "  \n",
    "    # Read the login credentials if provided\n",
    "    if cdsapirc_file:\n",
    "        with open(cdsapirc_file, 'r') as f:\n",
    "            credentials = yaml.safe_load(f)\n",
    "        kwargs = {'url': credentials['url'],\n",
    "                  'key': credentials['key'],\n",
    "                  'verify': credentials['url'].startswith('https://ads.')}\n",
    "    else:\n",
    "        kwargs = {}\n",
    "  \n",
    "    client = cdsapi.Client(**kwargs)\n",
    "    client.retrieve(\n",
    "        'cams-europe-air-quality-forecasts-optimised-at-observation-sites',\n",
    "        request,\n",
    "        data_file)\n",
    "  \n",
    "  \n",
    "def read_data(data_file, stations):\n",
    "    \"\"\"Read the downloaded zip file and return the station metadata and\n",
    "       concentration data as Pandas DataFrames\"\"\"\n",
    "  \n",
    "    data = {}\n",
    "  \n",
    "    # Loop over zip file contents\n",
    "    with zipfile.ZipFile(data_file) as zip:\n",
    "        for name in sorted(zip.namelist()):\n",
    "            with zip.open(name) as f:\n",
    "  \n",
    "                if name.startswith('station_list'):\n",
    "  \n",
    "                    # Read the station metadata file\n",
    "                    date_fmt = '%Y-%m-%d'\n",
    "                    station_data = pd.read_csv(f, sep=';',\n",
    "                                               keep_default_na=False)\n",
    "  \n",
    "                    # Remove metadata for stations we're not interested in\n",
    "                    if stations:\n",
    "                        station_data = station_data[\n",
    "                            station_data.id.isin(stations)]\n",
    "  \n",
    "                    # Set missing end dates to a date far in the future\n",
    "                    no_end = (station_data.date_end == '')\n",
    "                    station_data.loc[no_end, 'date_end'] = '2099-01-01'\n",
    "  \n",
    "                    # Parse start and end dates into datetime objects\n",
    "                    station_data.date_start = pd.to_datetime(\n",
    "                        station_data.date_start,\n",
    "                        format=date_fmt)\n",
    "                    station_data.date_end = pd.to_datetime(\n",
    "                        station_data.date_end,\n",
    "                        format=date_fmt)\n",
    "  \n",
    "                else:\n",
    "  \n",
    "                    # Read the data file\n",
    "                    df = pd.read_csv(f, sep=';', parse_dates=['datetime'],\n",
    "                                     infer_datetime_format=True)\n",
    "  \n",
    "                    # Remove data for stations we're not interested in\n",
    "                    if stations:\n",
    "                        df = df[df.station_id.isin(stations)]\n",
    "  \n",
    "                    # Get the name of the column containing concentration so we\n",
    "                    # can group data by raw/mos type\n",
    "                    data_col = [c for c in df.columns if c.startswith('conc_')]\n",
    "                    assert len(data_col) == 1\n",
    "                    data_col = data_col[0]\n",
    "                    if data_col not in data:\n",
    "                        data[data_col] = []\n",
    "  \n",
    "                    data[data_col].append(df)\n",
    "  \n",
    "    # Merge the data frames\n",
    "    merged_data = None\n",
    "    for data_col in list(data.keys()):\n",
    "  \n",
    "        # Concatenate all times/countries/species\n",
    "        x = pd.concat(data[data_col])\n",
    "  \n",
    "        # Merge raw and mos into combined records\n",
    "        if merged_data is None:\n",
    "            merged_data = x\n",
    "        else:\n",
    "            merged_data = merged_data.merge(x, how='outer', validate='1:1',\n",
    "                                            on=[c for c in x.columns\n",
    "                                                if c != data_col])\n",
    "  \n",
    "    return station_data, merged_data\n",
    "  \n",
    "  \n",
    "def station_plot(station, data, style):\n",
    "    \"\"\"Make a series of plots for a station - one for each species\"\"\"\n",
    "  \n",
    "    # Species to plot\n",
    "    allspecies = data.species.unique()\n",
    "  \n",
    "    # Create the figure\n",
    "    nplotsx = ceil(sqrt(len(allspecies)))\n",
    "    nplotsy = ceil(len(allspecies) / nplotsx)\n",
    "    fig = plt.figure(figsize=(nplotsx*8, nplotsy*5))\n",
    "    fig.subplots_adjust(hspace=0.35)\n",
    "  \n",
    "    # Plot each species\n",
    "    for iplot, species in enumerate(allspecies):\n",
    "        ax = plt.subplot(nplotsy, nplotsx, iplot + 1)\n",
    "  \n",
    "        # Extract data for just this species\n",
    "        sdata = data[data.species == species]\n",
    "  \n",
    "        plot_species(station, sdata, style, ax)\n",
    "  \n",
    "    plt.show()\n",
    "  \n",
    "  \n",
    "def plot_species(station, data, style, ax):\n",
    "    \"\"\"Make a plot for one species at one station\"\"\"\n",
    "  \n",
    "    allspecies = data.species.unique()\n",
    "    assert len(allspecies) == 1\n",
    "  \n",
    "    # Plotting both raw and mos-corrected or just one?\n",
    "    types = [t for t in ['raw', 'mos']\n",
    "             if f'conc_{t}_micrograms_per_m3' in data.columns]\n",
    "  \n",
    "    # Plot different lines for each post-processing type and lead time day\n",
    "    for type in types:\n",
    "        lead_time_day = data.lead_time_hour // 24\n",
    "        for day in lead_time_day.unique():\n",
    "            dt = data[lead_time_day == day]\n",
    "  \n",
    "            ax.plot(dt.datetime, dt[f'conc_{type}_micrograms_per_m3'],\n",
    "                    label=f'{type} forecast D+{day}',\n",
    "                    **prefs(type, day, style))\n",
    "  \n",
    "    # Nicer date labels\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H\\n%a %d'))\n",
    "  \n",
    "    ax.set_ylabel('$\\mu$g / m$^3$')\n",
    "  \n",
    "    date_range = 'from ' + ' to '.join(data.datetime.iat[i].strftime('%Y-%m-%d')\n",
    "                                       for i in [0, -1])\n",
    "    ax.set_title(\n",
    "        ('{species} at {station} (lat={lat}, lon={lon}, altitude={alt}m)\\n'\n",
    "         '{dates}').format(species=allspecies[0],\n",
    "                           station=station.id,\n",
    "                           lat=station.lat,\n",
    "                           lon=station.lon,\n",
    "                           alt=station.alt,\n",
    "                           dates=date_range))\n",
    "  \n",
    "    ax.legend()\n",
    "  \n",
    "  \n",
    "def prefs(type, leadtime, style):\n",
    "    \"\"\"Return pyplot.plot() keyword arguments for given type and leadtime\"\"\"\n",
    "  \n",
    "    return {**style.get('type', {}).get(type, {}),\n",
    "            **style.get('leadtime_day', {}).get(leadtime, {})}\n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  \n",
    "    # The ADS credentials can be passed as an argument if they're not stored in\n",
    "    # the default location\n",
    "    cdsapirc_file = sys.argv[1] if len(sys.argv) > 1 else None\n",
    "  \n",
    "    main(cdsapirc_file=cdsapirc_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
